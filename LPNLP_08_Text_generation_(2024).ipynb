{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Droushb/NULP_NLP/blob/main/LPNLP_08_Text_generation_(2024).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-tiVIhwDZgF"
      },
      "source": [
        "# Генерація тексту\n",
        "\n",
        "У цій роботі ми використаємо мовну модель для генерації тексту.\n",
        "\n",
        "У класичної мовної моделі є два взаємопов'язані визначення:\n",
        "\n",
        "1. Оцінити ймовірність вхідного тексту.\n",
        "2. Видати ймовірнісний розподіл наступного слова для даного префіксу.\n",
        "\n",
        "Для генерації тексту нам ідеально підходить друге визначення."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5cdtYXg9ARX"
      },
      "source": [
        "## Початок роботи\n",
        "\n",
        "Будь ласка, заповніть поля `EMAIL`, `NAME` та `GROUP` нижче:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --ignore-installed http://nlp.band/static/pypy/lpnlp-2023.10.2-py3-none-any.whl"
      ],
      "metadata": {
        "id": "sEbiQ-LYNANN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "vuKrzAhF9DS9",
        "outputId": "c9635fe1-cc1c-4a5a-d66a-cc42eb0fd5d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Удачі!\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "# FILL-IN:\n",
        "#-----------------------------------------------------------------------\n",
        "EMAIL = \"bohdan.drushkevych.kn.2021@lpnu.ua\"  # заповніть вашим значенням\n",
        "################################################################################\n",
        "\n",
        "import lpnlp\n",
        "\n",
        "lab = lpnlp.start(email=EMAIL, lab=\"text_generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIrY-_DL9XRv"
      },
      "source": [
        "### Завантаження моделі\n",
        "\n",
        "Тренування мовної моделі з нуля займає багато часу: від кількох годин або днів для маленьких та середніх моделей й аж до кількох місяців чи навіть років для великих. Звичайно, за рахунок розпаралелювання тренування на багатьох GPU, реальний час рідко буває більше місяця-двох.\n",
        "\n",
        "Для цієї роботи (як і в реальному житті), ми візьмемо претреновану модель. Хтось натренував її за нас. В данному випадку, візьмемо GPT-2 від компанії OpenAI.\n",
        "\n",
        "Ця модель, як і безліч інших, зберігається на [HuggingFace Models](https://huggingface.co/models). Завантажити та працювати з нею зручно через бібліотеку [HuggingFace Transformers](https://github.com/huggingface/transformers/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "oTaN8J8w4ddP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oadzovl02gxh"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpeVt8kn76PY"
      },
      "outputs": [],
      "source": [
        "model = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Токенізація\n",
        "\n",
        "Кожна модель має словник токенів, з яким вона тренувалася, та правила токенізації (розбиття тексту на токени). Обов'язково слід використовувати той самий словник та метод.\n",
        "\n",
        "Бібліотека transformers вміє робити це для кожної підтримуваної моделі."
      ],
      "metadata": {
        "id": "vaZDEIu85NJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "KTKW10u75soH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перевіримо токенізатор:"
      ],
      "metadata": {
        "id": "9NHX7-Rk5zwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Hello, LP NLP!\"\n",
        "token_ids = tokenizer.encode(input_text)\n",
        "token_ids"
      ],
      "metadata": {
        "id": "7Nc_Ntq659R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(token_ids)"
      ],
      "metadata": {
        "id": "JV8NyH1q6I-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рідкісні слова будуть розбиті на підслова:"
      ],
      "metadata": {
        "id": "7SFJkiCn60kS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(tokenizer.encode(\"My name is Oleksiy Syvokon\"))"
      ],
      "metadata": {
        "id": "RxmMsUgo6u7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3Uh8W7wMpwE"
      },
      "source": [
        "## Перевірка моделі -- один крок ітерації"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LiBLRgyNu32"
      },
      "source": [
        "Ми генеруватимемо текст в циклі токен за токеном, зліва направо. Але для початку розберімо, як виглядає один крок такого циклу.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9cbLyy3N-u9"
      },
      "source": [
        "На кожному кроці на вхід моделі подаємо ті токени, які вже згенеровано.\n",
        "\n",
        "На першому кроці у нас ще нічого не згенеровано. Тому починаємо зі токену `<BOS>` (\"begin of sentence\"). В різних моделях він виглядає по-різному. Подивимося, що в GPT-2:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.bos_token"
      ],
      "metadata": {
        "id": "xRVmoPoj0-IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.bos_token_id"
      ],
      "metadata": {
        "id": "bqnkWMCO3FOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6z8l1TlRQa-"
      },
      "outputs": [],
      "source": [
        "# Формуємо вхідний батч. Майже завжди для більшої ефективності\n",
        "# нейронні мережі очікують на вхід кілька незалежних речень\n",
        "# (або зображень у випадку з комп'ютерним зором)\n",
        "#\n",
        "# В нашій роботі ми завжди працюємо лише з одним реченням,\n",
        "# тож розмір батча дорівнює одинці. Але все одно маємо\n",
        "# оформити вхід як матрицю:\n",
        "input_ = torch.LongTensor([[tokenizer.bos_token_id]])\n",
        "input_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSiBQkJJSGue"
      },
      "outputs": [],
      "source": [
        "# Нарешті робимо крок генерації\n",
        "output = model(input_, return_dict=True)\n",
        "\n",
        "# На виході модель повертає вектор з logits -- ненормалізованими\n",
        "# ймовірностями кожного токена в словнику\n",
        "output.logits.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чому маємо саме таку розмірність?\n",
        "\n",
        "* Перша одиниця -- розмір батча, тобто кількість вхідних речень.\n",
        "* Друга одиниця -- це довжина вхідної послідовності. На першому кроці ми подали лише один токен (bos_token)\n",
        "* 50257 -- це розмір словника"
      ],
      "metadata": {
        "id": "Ckux9YLV4viE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "P_YkZjZl5MoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNoTYS-1Sr2y"
      },
      "source": [
        "Поки що модель повернула не ймовірності, а просто якісь числа. Їм треба нормалізувати функцією softmax:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs = torch.softmax(output.logits, dim=-1)"
      ],
      "metadata": {
        "id": "GNlOw5qT3wXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMCovI7uS1N8"
      },
      "outputs": [],
      "source": [
        "# Розмірність має збігатися з розміром словника\n",
        "probs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1H8iIaSTHJl"
      },
      "outputs": [],
      "source": [
        "# Сума ймовірностей має дорівнювати 1.0\n",
        "probs.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ймовірності можуть ставати дуже маленькими та причиняти проблеми у зв'язку з обмеженою точністю float чисел. Тому прийнято працювати з логарифмами ймовірностей."
      ],
      "metadata": {
        "id": "jUzNuPn15o7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_probs = torch.log_softmax(output.logits, dim=-1)\n",
        "\n",
        "# Щоб перейти до звичайних ймовірностей, маємо зробити експоненціювання\n",
        "log_probs.exp().sum()"
      ],
      "metadata": {
        "id": "sAN57ZiW5-75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Кожному слову в словнику відповідає своя ймовірність бути побаченим\n",
        "після заданого префікса. Префіксом у нас поки що був лише одни `bos_token`.\n"
      ],
      "metadata": {
        "id": "ZYdd1kQv6kYU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA1iznWPTdK4"
      },
      "outputs": [],
      "source": [
        "# Яка ймовірність, що речення почнеться зі слів \"I\", \"red\", \"Why\"?\n",
        "for word in (\"I\", \"red\", \"Why\"):\n",
        "    index = tokenizer.convert_tokens_to_ids(word)\n",
        "    prob = log_probs[0, 0, index].exp()\n",
        "    print(f\"P({word}) = {prob}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaoCr91RTw4Z"
      },
      "source": [
        "Тепер ми маємо ймовірностний розподіл по словнику. Можемо обрати слово, яке вважатимемо згенерованим. Тут можливі кілька стратегій, які ми розглянемо в наступних розділах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUbajoW_-ClA"
      },
      "source": [
        "## Greedy decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yOkcB3FUd3N"
      },
      "source": [
        "Найпростіший (але й не дуже цікавий) спосіб -- це завжди обирати токен з найбільшою ймовірністю:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG7QsAjPUoHW"
      },
      "outputs": [],
      "source": [
        "next_token_id = log_probs.argmax()\n",
        "next_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z87pe4Y0VJ1W"
      },
      "outputs": [],
      "source": [
        "probs[-1, -1, 198]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaZRZWpAUtNY"
      },
      "outputs": [],
      "source": [
        "tokenizer.convert_ids_to_tokens([next_token_id])  # Наш перший згенерований токен"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVB5pXOoX29S"
      },
      "source": [
        "Зберемо код докупи та додамо цикл. В циклі ми продовжуватимемо генерувати текст токен за токеном, поки не настане одна з двох умов:\n",
        "1. Модель видала спецальний токен `eos_token` (end of sentence)\n",
        "2. Довжина згенерованого тексту перевищила певний поріг `max_len`\n",
        "\n",
        "У хорошої моделі в більшості випадків має спрацьовувати перша умова зупинки. Проте іноді модель може впасти в безкінчений цикл. Щоб цьому запобігти, маємо другу умову."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG8nI5WG-Qb5"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def greedy_decode(model, tokenizer, max_len=50):\n",
        "    start_index = tokenizer.bos_token_id\n",
        "    result = [start_index]\n",
        "\n",
        "    while len(result) < max_len:\n",
        "\n",
        "        # Передбачення ймовірностней наступного токена\n",
        "        input_ = torch.LongTensor([result])\n",
        "        output = model(input_)\n",
        "        probs = torch.log_softmax(output.logits[0, -1], dim=-1)\n",
        "\n",
        "        # Обираємо токен, що має найбільшу ймовірність\n",
        "        token_index = probs.argmax()\n",
        "\n",
        "        # Зупиняємося, якщо досягли кінця\n",
        "        if token_index == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        # Додаємо обраний токен в згенерований текст\n",
        "        result.append(token_index.item())\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "generated_token_ids = greedy_decode(model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Маємо список згенерованих індексів токенів, який починається ось так:"
      ],
      "metadata": {
        "id": "4myF4KrG-rxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_token_ids[:10]"
      ],
      "metadata": {
        "id": "FXp0P-Yz-0UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перетворимо їх в текст:"
      ],
      "metadata": {
        "id": "ANlG9v8--8Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(generated_token_ids)\n",
        "tokens[:10]"
      ],
      "metadata": {
        "id": "t7X6m4vs91IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = \"\".join(tokens)\n",
        "generated_text"
      ],
      "metadata": {
        "id": "vde2_-J1_JDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv0i3pqdJ3Bn"
      },
      "outputs": [],
      "source": [
        "lab.checkpoint(\"greedy decode\", generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GN1tc5kYsOk"
      },
      "source": [
        "### Примітка: Byte-pair encoding (BPE)\n",
        "\n",
        "Наша модель використовує subword токенізацію, а саме byte-pair encoding (BPE). В сучасному NLP це найрозповсюдженіший спосіб токенізації. Детально можете подивитися в [цьому відео](https://www.youtube.com/watch?v=tOMjTCO0htA).\n",
        "\n",
        "Для наших цілей зараз важливо, що BPE заміняє пробіли на спеціальні Unicode-символи \"Ġ\". Серед інших моделей широко поширений варіант \"▁\" (зверніть увагу, це не звичайний символ підкреслення \"_\"). Щоб отримати чистий текст, треба виконати наступну заміну:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5S_BPCLY_V9h"
      },
      "outputs": [],
      "source": [
        "def bpe_decode(s):\n",
        "    result = s.replace(\"Ġ\", \" \")\n",
        "    result = result.replace(\"Ċ\", \"\\n\")\n",
        "    return result\n",
        "\n",
        "bpe_decode(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Але краще довіритися токенізатору й зробити цю роботу за нас:"
      ],
      "metadata": {
        "id": "42kMbpie_3cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_tokens_to_string(tokens)"
      ],
      "metadata": {
        "id": "oycMPnuo__po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK0wKs4IAk4l"
      },
      "source": [
        "## Generic decoding function\n",
        "\n",
        "Обирати слово з найбільшою ймовірністю -- не найкращий варіант для генерації тексту хоча б тому, що він завжди детерміновано призводить до однієї послідовності. Нижче ми подивимося на цікавіші альтернативи.\n",
        "\n",
        "Цикл генерації залишиться той самий, що і в `greedy_decode()`. Відрізнятися буде лише один рядок -- той, в якому ми приймали рішення, яке слово обрати. Для зручності, винесемо цей рядок в окрему функцію. Ця функція прийматиме на вхід ймовірностний розподіл по словнику і повертає обраний токен."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KQ_8Mky9enZ"
      },
      "outputs": [],
      "source": [
        "def greedy_choice(probs):\n",
        "    return probs.argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmH11o729oQL"
      },
      "source": [
        "Функцію генерації також трохи переробимо.\n",
        "\n",
        "По-перше, додамо параметр `sample_fn` -- це має бути функція, яка обирає слово з ймовірностного розподілу, наприклад, `greedy_choice`.\n",
        "\n",
        "По-друге, для зручності виконуватимемо BPE декодинг у середині функції генерації."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eEGscC5Amzw"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, tokenizer, sample_fn, max_len=50, bpe_decode=True):\n",
        "    start_index = tokenizer.bos_token_id\n",
        "    result = [start_index]\n",
        "\n",
        "    while len(result) < max_len:\n",
        "\n",
        "        # Передбачення ймовірностней наступного токена\n",
        "        input_ = torch.LongTensor([result])\n",
        "        output = model(input_)\n",
        "        log_probs = torch.log_softmax(output.logits[0, -1], dim=-1)\n",
        "\n",
        "        # Обираємо токен, що має найбільшу ймовірність\n",
        "        token_index = sample_fn(log_probs.exp())     # <---------------- цей рядок змінено\n",
        "\n",
        "        # Зупиняємося, якщо досягнули кінця\n",
        "        if token_index == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        # Додаємо обраний токен в згенерований текст\n",
        "        result.append(token_index.item())\n",
        "\n",
        "    if bpe_decode:\n",
        "        tokens = tokenizer.convert_ids_to_tokens(result)\n",
        "        result = tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "generate(model, tokenizer, greedy_choice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXHiIrfZAnpW"
      },
      "source": [
        "## Simple sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bbdLT96Bd8c"
      },
      "source": [
        "Перший альтернатива -- це sampling. Тепер ми обираємо наступний токен випадково, але пропорційно до ймовірностей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01Y-djIkAsGB"
      },
      "outputs": [],
      "source": [
        "def simple_sample(probs):\n",
        "    return torch.multinomial(probs, num_samples=1)[0]\n",
        "\n",
        "# Згенеруємо 5 речень\n",
        "for i in range(1, 6):\n",
        "    result = generate(model, tokenizer, simple_sample)\n",
        "    print(f\"#{i}: {result}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwJG_jGmBXCU"
      },
      "source": [
        "## Sampling with temperature\n",
        "\n",
        "Ми також можемо впливати на генерацію параметром температури softmax.\n",
        "\n",
        "Більші значення температури призводять до того, що різниця між ймовірностями токенів зменшується, тобто розподіл стає більш рівномірним. На практиці це означає, що менш ймовірні варіанти обиратимуться частіше і згенерований текст може бути цікавішим. Однак якщо продовжувати піднімати температуру, то текст спочатку втратить зв'язність, далі почнуть розпадатися слова та граматичність.\n",
        "\n",
        "Менші значення температури змінюють розподіл таким чином, що основна ймовірніста маса припадає на невелику кількість топових токенів. При температурі 0 вся ймовірність дістанеться одному токену й семплінг перетвориться на greedy decoding.\n",
        "\n",
        "Згенеруємо тексти з різною температурою:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "0Y_J1eS3BfTF",
        "outputId": "f0da7279-8b24-4610-bbdf-9510850a4b82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling with temperature=0.1\n",
            "<|endoftext|>\n",
            "The U.S. Department of Justice has been investigating the use\n",
            "\n",
            "Sampling with temperature=0.3\n",
            "<|endoftext|>The world's largest data center is the world's largest data center,\n",
            "\n",
            "Sampling with temperature=0.5\n",
            "<|endoftext|>\"I'm not sure what to say, but I'm just trying\n",
            "\n",
            "Sampling with temperature=0.8\n",
            "<|endoftext|>\"It makes you sigh out loud: You're right. It makes\n",
            "\n",
            "Sampling with temperature=1.0\n",
            "<|endoftext|>Last Sunday, the vast majority of voters were not happy with the election\n",
            "\n",
            "Sampling with temperature=1.25\n",
            "<|endoftext|>'t V1 God lessons from Jake Liveseren at Cloud Imperium Games.\n",
            "\n",
            "Sampling with temperature=1.5\n",
            "<|endoftext|> 1925 notice showing street developements in Palit located off Hinds Rivers\n",
            "\n",
            "Sampling with temperature=2.0\n",
            "<|endoftext|>Wait Nothing Second Abs[\"Rockbanks Mysterious Sheriff Ron impeachment of dictator 95\n",
            "\n",
            "Sampling with temperature=3.0\n",
            "<|endoftext|>Asian greens recalling pins227 offer URIHyp literacy27oser Expressionaverage ape\n",
            "\n",
            "Sampling with temperature=5.0\n",
            "<|endoftext|> referringFN Ireland inqu vigobos Hamb crashed created wedge dissuirs huntersworker\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for temperature in (0.1, 0.3, 0.5, 0.8, 1.0, 1.25, 1.5, 2.0, 3.0, 5.0):\n",
        "\n",
        "    def sample_with_temp(probs):\n",
        "        updated_probs = probs.log().div(temperature).exp()\n",
        "        return simple_sample(updated_probs)\n",
        "\n",
        "    print(f\"Sampling with temperature={temperature}\")\n",
        "    result = generate(model, tokenizer, sample_with_temp, max_len=15)\n",
        "    print(result)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Яке значення `temperature` здається вам оптимальною?\n",
        "lab.checkpoint(\"softmax temperature\", 1.0)"
      ],
      "metadata": {
        "id": "ShTm-dddw6s9",
        "outputId": "80934379-bd2e-4fd0-9388-01e36e4fdac1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Відповідь правильна ✅\n",
            "Окей, добре\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO9-xZxRBkIB"
      },
      "source": [
        "## Top-k sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "HqPZTXxZCQdc"
      },
      "outputs": [],
      "source": [
        "def top_k_sampling(probs, k):\n",
        "    topk = probs.topk(k)\n",
        "    index = torch.multinomial(topk.values, num_samples=1)[0]\n",
        "#     print(f\"Top {k} words take {topk.values.sum():%} probability mass\")\n",
        "    return topk.indices[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Q2BAZ9mnCRGC",
        "outputId": "e189f99c-f076-4c37-a83e-9882af049ad1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#1: <|endoftext|>\n",
            "The US has sent troops to support forces fighting Isis in the north-eastern town of\n",
            "\n",
            "#2: <|endoftext|>This is a rush transcript. Copy may not be in its final form.\n",
            "\n",
            "AMY GOODMAN\n",
            "\n",
            "#3: <|endoftext|>A group of scientists has found evidence that a common, low level of stress may be linked to\n",
            "\n",
            "#4: <|endoftext|>Bethany Moulins of the University of California at San Diego says that her husband's\n",
            "\n",
            "#5: <|endoftext|>A new study by researchers from the U.S. Environmental Protection Agency (EPA) found that\n",
            "\n"
          ]
        }
      ],
      "source": [
        "k = 15\n",
        "sample_fn = lambda probs: top_k_sampling(probs, k=k)\n",
        "for i in range(1, 6):\n",
        "    result = generate(model, tokenizer, sample_fn, max_len=20)\n",
        "    print(f\"#{i}: {result}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuJFHRsaCoAK"
      },
      "source": [
        "## Nucleus (top-p) sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "PUIZQgXhCU1l"
      },
      "outputs": [],
      "source": [
        "def nucleus_sampling(probs, max_p):\n",
        "    sorted_probs = probs.sort(descending=True)\n",
        "    cum_prob = 0.0\n",
        "    sample_indices = []\n",
        "    sample_probs = []\n",
        "    for i in range(0, len(sorted_probs.values)):\n",
        "        p = sorted_probs.values[i]\n",
        "        cum_prob += p\n",
        "        sample_probs.append(p)\n",
        "        sample_indices.append(sorted_probs.indices[i])\n",
        "        if cum_prob >= max_p:\n",
        "            break\n",
        "\n",
        "    index = torch.multinomial(torch.tensor(sample_probs), num_samples=1)\n",
        "    return sample_indices[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "FXuy2S8tCq_Z",
        "outputId": "dae87edf-d1a8-4409-ce1a-2248ba0f399c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0.0: <|endoftext|>\n",
            "The first time I saw the new version of the game, I was so excited. I\n",
            "\n",
            "#0.1: <|endoftext|>\n",
            "The new report from the National Institute of Mental Health (NIMH) on mental health\n",
            "\n",
            "#0.3: <|endoftext|>In the past, we've seen the rapid rise of these microservices. We've seen them\n",
            "\n",
            "#0.5: <|endoftext|>8th August 2018\n",
            "\n",
            "4th August 2018\n",
            "\n",
            "6th August 2018\n",
            "\n",
            "5\n",
            "\n",
            "#0.6: <|endoftext|>In my last post, I talked about how if you go through the template with the STL,\n",
            "\n",
            "#0.8: <|endoftext|>Note: This page is based on a project from the previous project.\n",
            "\n",
            "Installation\n",
            "\n",
            "\n",
            "\n",
            "#1.0: <|endoftext|>LAS VEGAS (CBS) – A 13-year-old Lansing girl left the\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for max_p in (0.0, 0.1, 0.3, 0.5, 0.6, 0.8, 1.0):\n",
        "    sample_fn = lambda probs: nucleus_sampling(probs, max_p=max_p)\n",
        "    result = generate(model, tokenizer, sample_fn, max_len=20)\n",
        "    print(f\"#{max_p}: {result}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra-F6HyTCxcG"
      },
      "source": [
        "## Start from prompt\n",
        "\n",
        "До цього моменту ми генерували текст з нуля. Однак значно кориснішим є задача генерації тексту від певного префікса або \"підказки\" -- в англійській мові це називається \"prompt\".\n",
        "\n",
        "Prompt дозволить нам контролювати тематику згенерованого тексту або, як ми побачимо на іншій лекції, допоможе моделі виконувати різноманітні завдання."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "7mP3YWPfDI7Q"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, tokenizer, sample_fn, prompt, max_len=50, bpe_decode=True):\n",
        "\n",
        "    result = tokenizer.encode(prompt)     # <---- цей рядок додано\n",
        "\n",
        "    while len(result) < max_len:\n",
        "\n",
        "        # Передбачення ймовірностней наступного токена\n",
        "        input_ = torch.LongTensor([result])\n",
        "        output = model(input_)\n",
        "        log_probs = torch.log_softmax(output.logits[0, -1], dim=-1)\n",
        "\n",
        "        # Обираємо токен, що має найбільшу ймовірність\n",
        "        token_index = sample_fn(log_probs.exp())\n",
        "\n",
        "        # Зупиняємося, якщо досягнули кінця\n",
        "        if token_index == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        # Додаємо обраний токен в згенерований текст\n",
        "        result.append(token_index.item())\n",
        "\n",
        "    if bpe_decode:\n",
        "        tokens = tokenizer.convert_ids_to_tokens(result)\n",
        "        result = tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "tBf2m-x7DJ0r",
        "outputId": "4ea9a6bd-9952-4c76-b7ea-6336efddbc70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#1: One of my favorite things is the way the game is presented. It's like a game that you can see and feel and it's very easy to play and very easy to play. It's very easy to play and very easy to play.\n",
            "\n",
            "\n",
            "#2: One of my favorite things is the way the game is played. It's a very simple game. You can't really tell if you're going to win or lose, but you can see where you are. You can see what your opponent is doing\n",
            "\n",
            "#3: One of my favorite things is the fact that I can go back and forth between them. I've been in the business for a while now, and my wife and I are very close. I've been in the business for about 10 years now.\n",
            "\n",
            "#4: One of my favorite things is that it's so easy to get started with it. It's just so easy. It's like you're in a game of chess. It's like you're in an open world where there are lots of places to\n",
            "\n"
          ]
        }
      ],
      "source": [
        "k = 3\n",
        "sample_fn = lambda probs: top_k_sampling(probs, k=k)\n",
        "for i in range(1, 5):\n",
        "    result = generate(model, tokenizer, sample_fn, prompt=\"One of my favorite things is\")\n",
        "    print(f\"#{i}: {result}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "# Спробуйте змінити prompt на щось інше"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-shot класифікація\n",
        "\n",
        "Ми будемо досліджувати цю тему на наступній лекції. Але спробуємо застосувати мовну модель для задачі класифікації вже зараз. Хоча наша модель надто маленька й слабка для серйозного використання в zero-shot.\n"
      ],
      "metadata": {
        "id": "0qBd5PJlxP2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, greedy_choice, prompt=\"This film is awful! Actor play is terrible. The plot is dull. I would rate it as a\").splitlines()[0]\n"
      ],
      "metadata": {
        "id": "2-DXnTBUHurd",
        "outputId": "699d6220-86c7-4f9c-9945-a0e89c187bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This film is awful! Actor play is terrible. The plot is dull. I would rate it as a B+.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, greedy_choice, prompt=\"This film is amazing! The time flies by when you watch it. Definitely recommend! On the scale of 1 to 5, I would rate it as a\").splitlines()[0]\n"
      ],
      "metadata": {
        "id": "jceNhVHYJbMH",
        "outputId": "d618fa45-16f1-4dcc-8533-c4404ca5db93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This film is amazing! The time flies by when you watch it. Definitely recommend! On the scale of 1 to 5, I would rate it as a 5.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "cuK8D9uQJR5c",
        "outputId": "4b79ee14-4dca-4ce2-d403-5a7c0d00f15f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Відповідь правильна ✅\n",
            "Ця робота не має однієї правильної відповіді. Вважаємо лабу пройденою :) Формочка: https://tally.so/r/mZ81ve\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Готово!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "lab.answer(\"Готово!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Real world\n",
        "\n",
        "У цій роботі ми імплементували кілька методів декодінгу. Але, звичайно, все вже зроблено за нас.\n",
        "\n",
        "Подивіться на параметри функції [generate()](https://huggingface.co/docs/transformers/v4.21.1/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate) з бібліотеки transformers. Багато з них мають виглядати знайомими.\n",
        "\n",
        "Повний приклад використання:"
      ],
      "metadata": {
        "id": "Xy0OIkpIwn1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "prompt = \"I hope that in that practical class on text generation you\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, do_sample=True, top_p=0.8, max_length=50)\n",
        "\n",
        "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "nTdm0OLX0qfZ",
        "outputId": "478d954a-9675-4d83-ff13-eb058fafd8cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I hope that in that practical class on text generation you'll also be able to learn to draw with a pencil, and to make beautiful art on paper.\\n\\nMy name is Thomas and I am from California. I grew up in the Bay Area\"]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The End\n"
      ],
      "metadata": {
        "id": "Fj_v43SP0ooz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Сподіваюсь, ця лаба була не надто важкою :)\n",
        "lab.answer(\"Готово!\")"
      ],
      "metadata": {
        "id": "SVwIySR60N-J",
        "outputId": "81435577-e487-4ddf-b5a9-c0816fb2fdf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Відповідь правильна ✅\n",
            "Ця робота не має однієї правильної відповіді. Вважаємо лабу пройденою :) Формочка: https://tally.so/r/mZ81ve\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Готово!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}